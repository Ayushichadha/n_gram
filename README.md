# n_gram
building ngram language model.

implementing basics of machine learning - training, data splitting, hyperparameters, overfitting, evaluation 

basics of autoregressive language modelling - tokenisation, next token prediction, perplexity, sampling.

GPT is a very large ngram model, the only difference is it uses neural networks to calculate the probability of next token, whereas ngram uses counting.

Dataset: 32,032 names from ssa.gov for the year 2018 (just like Andrej Karpathy lectures for makemore series)

Notes: "Speech and Language Processing" by Jurafsky and Martin. (Chapter: Ngram models)
